{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Book Recommendation System\n",
    "\n",
    "This notebook implements a collaborative filtering recommendation system for books using deep learning. The system uses the Book-Crossing dataset to predict user ratings for books based on both user and book features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dot, Lambda\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "from numpy import genfromtxt\n",
    "from collections import defaultdict\n",
    "pd.set_option(\"display.precision\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from numpy import genfromtxt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import csv\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing and File Generation\n",
    "\n",
    "This section handles the initial data processing steps:\n",
    "1. Loads the original Book-Crossing dataset (books, users, and ratings)\n",
    "2. Samples 10% of the ratings for training\n",
    "3. Merges the data to create complete training examples\n",
    "4. Processes and encodes categorical features\n",
    "5. Generates necessary CSV files for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of merged (training) rows: 103130\n",
      "Row counts in generated files:\n",
      "content_item_train.csv: 103130\n",
      "content_user_train.csv: 103130\n",
      "content_y_train.csv: 103130\n",
      "Success: All generated files have the same number of rows.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import pickle\n",
    "\n",
    "def generate_csv_files():\n",
    "    \"\"\"Generate necessary CSV files from book, user, and ratings data using 10% of the ratings.\n",
    "       Each row in the output files corresponds to one rating (with its associated book and user features),\n",
    "       so all three files will have the same number of rows.\"\"\"\n",
    "    \n",
    "    # Load original data with proper quoting for books\n",
    "    books = pd.read_csv('data/BX_Books.csv', delimiter=';', encoding='latin-1', \n",
    "                        quoting=csv.QUOTE_NONNUMERIC)\n",
    "    users = pd.read_csv('data/BX-Users.csv', delimiter=';', encoding='latin-1')\n",
    "    ratings = pd.read_csv('data/BX-Book-Ratings.csv', delimiter=';', encoding='latin-1')\n",
    "    \n",
    "    # Sample 10% of the ratings (this is our primary unit of training)\n",
    "    ratings_sampled = ratings.sample(frac=0.1, random_state=42)\n",
    "    \n",
    "    # Merge the sampled ratings with books and users so that each row gets its corresponding features.\n",
    "    # This inner merge ensures that each row in the merged DataFrame is complete.\n",
    "    merged = pd.merge(ratings_sampled, books, on='ISBN', how='inner')\n",
    "    merged = pd.merge(merged, users, on='User-ID', how='inner')\n",
    "    \n",
    "    # At this point, every row in \"merged\" corresponds to one rating with its book and user features.\n",
    "    total_rows = len(merged)\n",
    "    print(f\"Number of merged (training) rows: {total_rows}\")\n",
    "    \n",
    "    # Extract the three dataframes from the merged DataFrame:\n",
    "    #   - y_train: the ratings\n",
    "    #   - item_train: the book features\n",
    "    #   - user_train: the user features\n",
    "    y_train_df = merged[['Book-Rating']].copy()\n",
    "    item_train_df = merged[['ISBN', 'Book-Title', 'Book-Author', 'Year-Of-Publication', 'Publisher']].copy()\n",
    "    user_train_df = merged[['User-ID', 'Location', 'Age']].copy()\n",
    "    \n",
    "    # Process item features:\n",
    "    item_train_df['Year-Of-Publication'] = pd.to_numeric(item_train_df['Year-Of-Publication'], errors='coerce')\n",
    "    item_train_df['Book-Title'] = pd.factorize(item_train_df['Book-Title'])[0]\n",
    "    item_train_df['Book-Author'] = pd.factorize(item_train_df['Book-Author'])[0]\n",
    "    item_train_df['Publisher'] = pd.factorize(item_train_df['Publisher'])[0]\n",
    "    item_train_df = item_train_df.fillna(0)\n",
    "    \n",
    "    # Process user features:\n",
    "    user_train_df['Location'] = pd.factorize(user_train_df['Location'])[0]\n",
    "    user_train_df = user_train_df.fillna(0)\n",
    "    \n",
    "    # Save CSV files with proper quoting to ensure consistency.\n",
    "    item_train_df.to_csv('data/content_item_train.csv', index=False, header=False, quoting=csv.QUOTE_NONNUMERIC)\n",
    "    user_train_df.to_csv('data/content_user_train.csv', index=False, header=False, quoting=csv.QUOTE_NONNUMERIC)\n",
    "    y_train_df.to_csv('data/content_y_train.csv', index=False, header=False)\n",
    "    \n",
    "    # Save header files.\n",
    "    with open('data/content_item_train_header.txt', 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['ISBN', 'Book-Title', 'Book-Author', 'Year-Of-Publication', 'Publisher'])\n",
    "    \n",
    "    with open('data/content_user_train_header.txt', 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['User-ID', 'Location', 'Age'])\n",
    "    \n",
    "    # Create content_item_vecs.csv from numeric columns of item_train_df.\n",
    "    item_vecs = item_train_df.select_dtypes(include=[np.number]).values\n",
    "    np.savetxt('data/content_item_vecs.csv', item_vecs, delimiter=',')\n",
    "    \n",
    "    # Create a book list file (here renamed to \"content_book_list.csv\") for reference.\n",
    "    # We use the original (non-factorized) book fields from the sampled books.\n",
    "    book_list = books[books['ISBN'].isin(merged['ISBN'])][['ISBN', 'Book-Title', 'Book-Author']]\n",
    "    book_list.to_csv('data/content_book_list.csv', index=False)\n",
    "    \n",
    "    # Create content_user_to_genre.pickle:\n",
    "    # Since we may not have explicit genre info, we construct a simple user-to-author preference dictionary.\n",
    "    user_to_author = {}\n",
    "    for _, row in merged.iterrows():\n",
    "        uid = row['User-ID']\n",
    "        author = row['Book-Author']  # Use the original author string if desired (or factorized version)\n",
    "        if uid not in user_to_author:\n",
    "            user_to_author[uid] = {}\n",
    "        user_to_author[uid][author] = user_to_author[uid].get(author, 0) + 1\n",
    "        \n",
    "    with open('data/content_user_to_genre.pickle', 'wb') as f:\n",
    "        pickle.dump(user_to_author, f)\n",
    "    \n",
    "    # Read back row counts (from the in-memory dataframes) to confirm consistency:\n",
    "    num_item_rows = len(item_train_df)\n",
    "    num_user_rows = len(user_train_df)\n",
    "    num_y_rows = len(y_train_df)\n",
    "    print(\"Row counts in generated files:\")\n",
    "    print(f\"content_item_train.csv: {num_item_rows}\")\n",
    "    print(f\"content_user_train.csv: {num_user_rows}\")\n",
    "    print(f\"content_y_train.csv: {num_y_rows}\")\n",
    "\n",
    "    if num_item_rows == num_user_rows == num_y_rows:\n",
    "        print(\"Success: All generated files have the same number of rows.\")\n",
    "    else:\n",
    "        print(\"Error: Mismatch in row counts!\")\n",
    "\n",
    "# Call the function to generate files.\n",
    "generate_csv_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Processing\n",
    "\n",
    "This section includes functions to:\n",
    "1. Load the preprocessed data files\n",
    "2. Scale the features appropriately\n",
    "3. Split the data into training and test sets\n",
    "4. Display sample data for verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "from collections import defaultdict\n",
    "import csv\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"Load all necessary data for the book recommendation system\"\"\"\n",
    "    \n",
    "    item_train = genfromtxt('./data/content_item_train.csv', delimiter=',', \n",
    "                           dtype=float, skip_header=0, filling_values=0)\n",
    "    user_train = genfromtxt('./data/content_user_train.csv', delimiter=',', \n",
    "                           dtype=float, skip_header=0, filling_values=0)\n",
    "    y_train    = genfromtxt('./data/content_y_train.csv', delimiter=',', \n",
    "                           dtype=float, skip_header=0, filling_values=0)\n",
    "    \n",
    "    with open('./data/content_item_train_header.txt', newline='') as f:\n",
    "        item_features = list(csv.reader(f))[0]\n",
    "    with open('./data/content_user_train_header.txt', newline='') as f:\n",
    "        user_features = list(csv.reader(f))[0]\n",
    "        \n",
    "    item_vecs = genfromtxt('./data/content_item_vecs.csv', delimiter=',')\n",
    "    \n",
    "    book_dict = defaultdict(dict)\n",
    "    count = 0\n",
    "    with open('./data/content_book_list.csv', newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "        for line in reader:\n",
    "            if count == 0:\n",
    "                count += 1\n",
    "            else:\n",
    "                count += 1\n",
    "                book_id = line[0]  # ISBN\n",
    "                book_dict[book_id][\"title\"] = line[1]  # Book-Title\n",
    "                book_dict[book_id][\"author\"] = line[2]  # Book-Author\n",
    "\n",
    "    with open('./data/content_user_to_genre.pickle', 'rb') as f:\n",
    "        user_to_genre = pickle.load(f)\n",
    "\n",
    "    return(item_train, user_train, y_train, item_features, user_features, item_vecs, book_dict, user_to_genre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_train, user_train, y_train, item_features, user_features, item_vecs, book_dict, user_to_genre = load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training vectors: 66003\n",
      "Number of user vectors: 66003\n",
      "Number of y vectors: 66003\n"
     ]
    }
   ],
   "source": [
    "num_user_features = user_train.shape[1] - 1  # remove userid, rating count and ave rating during training\n",
    "num_item_features = item_train.shape[1] - 1  # remove movie id at train time\n",
    "uvs = 3  # user genre vector start\n",
    "ivs = 3  # item genre vector start\n",
    "u_s = 1  # start of columns to use in training, user\n",
    "i_s = 1  # start of columns to use in training, items\n",
    "scaledata = True  # applies the standard scalar to data if true\n",
    "print(f\"Number of training vectors: {len(item_train)}\")\n",
    "print(f\"Number of user vectors: {len(user_train)}\")\n",
    "print(f\"Number of y vectors: {len(y_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_str(ifeatures, smax):\n",
    "    ofeatures = []\n",
    "    for s in ifeatures:\n",
    "        if ' ' not in s:  # skip string that already have a space            \n",
    "            if len(s) > smax:\n",
    "                mid = int(len(s)/2)\n",
    "                s = s[:mid] + \" \" + s[mid:]\n",
    "        ofeatures.append(s)\n",
    "    return(ofeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tabulate\n",
    "def pprint_train(x_train, features,  vs, u_s, maxcount = 5, user=True):\n",
    "    \"\"\" Prints user_train or item_train nicely \"\"\"\n",
    "    if user:\n",
    "        flist = [\".0f\",\".0f\",\".1f\", \n",
    "                 \".1f\", \".1f\", \".1f\", \".1f\",\".1f\",\".1f\", \".1f\",\".1f\",\".1f\", \".1f\",\".1f\",\".1f\",\".1f\",\".1f\"]\n",
    "    else:\n",
    "        flist = [\".0f\",\".0f\",\".1f\", \n",
    "                 \".0f\",\".0f\",\".0f\", \".0f\",\".0f\",\".0f\", \".0f\",\".0f\",\".0f\", \".0f\",\".0f\",\".0f\",\".0f\",\".0f\"]\n",
    "\n",
    "    head = features[:vs]\n",
    "    if vs < u_s: print(\"error, vector start {vs} should be greater then user start {u_s}\")\n",
    "    for i in range(u_s):\n",
    "        head[i] = \"[\" + head[i] + \"]\"\n",
    "    genres = features[vs:]\n",
    "    hdr = head + genres\n",
    "    disp = [split_str(hdr, 5)]\n",
    "    count = 0\n",
    "    for i in range(0,x_train.shape[0]):\n",
    "        if count == maxcount: break\n",
    "        count += 1\n",
    "        disp.append( [ \n",
    "                      x_train[i,0].astype(int),  \n",
    "                      x_train[i,1].astype(int),   \n",
    "                      x_train[i,2].astype(float), \n",
    "                      *x_train[i,3:].astype(float)\n",
    "                    ])\n",
    "    table = tabulate.tabulate(disp, tablefmt='html',headers=\"firstrow\", floatfmt=flist, numalign='center')\n",
    "    return(table)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th style=\"text-align: center;\"> [Use r-ID] </th><th style=\"text-align: center;\"> Loca tion </th><th style=\"text-align: center;\"> Age </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td style=\"text-align: center;\">     0      </td><td style=\"text-align: center;\">     0     </td><td style=\"text-align: center;\"> 1.5 </td></tr>\n",
       "<tr><td style=\"text-align: center;\">     1      </td><td style=\"text-align: center;\">     0     </td><td style=\"text-align: center;\">-1.3 </td></tr>\n",
       "<tr><td style=\"text-align: center;\">     1      </td><td style=\"text-align: center;\">     0     </td><td style=\"text-align: center;\">-1.3 </td></tr>\n",
       "<tr><td style=\"text-align: center;\">     -1     </td><td style=\"text-align: center;\">     0     </td><td style=\"text-align: center;\">-0.3 </td></tr>\n",
       "<tr><td style=\"text-align: center;\">     -1     </td><td style=\"text-align: center;\">     0     </td><td style=\"text-align: center;\">-1.3 </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "'<table>\\n<thead>\\n<tr><th style=\"text-align: center;\"> [Use r-ID] </th><th style=\"text-align: center;\"> Loca tion </th><th style=\"text-align: center;\"> Age </th></tr>\\n</thead>\\n<tbody>\\n<tr><td style=\"text-align: center;\">     0      </td><td style=\"text-align: center;\">     0     </td><td style=\"text-align: center;\"> 1.5 </td></tr>\\n<tr><td style=\"text-align: center;\">     1      </td><td style=\"text-align: center;\">     0     </td><td style=\"text-align: center;\">-1.3 </td></tr>\\n<tr><td style=\"text-align: center;\">     1      </td><td style=\"text-align: center;\">     0     </td><td style=\"text-align: center;\">-1.3 </td></tr>\\n<tr><td style=\"text-align: center;\">     -1     </td><td style=\"text-align: center;\">     0     </td><td style=\"text-align: center;\">-0.3 </td></tr>\\n<tr><td style=\"text-align: center;\">     -1     </td><td style=\"text-align: center;\">     0     </td><td style=\"text-align: center;\">-1.3 </td></tr>\\n</tbody>\\n</table>'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pprint_train(user_train, user_features, uvs,  u_s, maxcount=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th style=\"text-align: center;\"> [IS BN] </th><th style=\"text-align: center;\"> Book- Title </th><th style=\"text-align: center;\"> Book- Author </th><th style=\"text-align: center;\"> Year-Of-P ublication </th><th style=\"text-align: center;\"> Publ isher </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td style=\"text-align: center;\">    0    </td><td style=\"text-align: center;\">      0      </td><td style=\"text-align: center;\">     -0.8     </td><td style=\"text-align: center;\">          0           </td><td style=\"text-align: center;\">     -0     </td></tr>\n",
       "<tr><td style=\"text-align: center;\">    0    </td><td style=\"text-align: center;\">      0      </td><td style=\"text-align: center;\">     -0.6     </td><td style=\"text-align: center;\">          0           </td><td style=\"text-align: center;\">     -0     </td></tr>\n",
       "<tr><td style=\"text-align: center;\">    0    </td><td style=\"text-align: center;\">      0      </td><td style=\"text-align: center;\">     0.2      </td><td style=\"text-align: center;\">          0           </td><td style=\"text-align: center;\">     -0     </td></tr>\n",
       "<tr><td style=\"text-align: center;\">    0    </td><td style=\"text-align: center;\">     -1      </td><td style=\"text-align: center;\">     -0.8     </td><td style=\"text-align: center;\">          0           </td><td style=\"text-align: center;\">     -0     </td></tr>\n",
       "<tr><td style=\"text-align: center;\">    0    </td><td style=\"text-align: center;\">      1      </td><td style=\"text-align: center;\">     -0.7     </td><td style=\"text-align: center;\">          0           </td><td style=\"text-align: center;\">     -0     </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "'<table>\\n<thead>\\n<tr><th style=\"text-align: center;\"> [IS BN] </th><th style=\"text-align: center;\"> Book- Title </th><th style=\"text-align: center;\"> Book- Author </th><th style=\"text-align: center;\"> Year-Of-P ublication </th><th style=\"text-align: center;\"> Publ isher </th></tr>\\n</thead>\\n<tbody>\\n<tr><td style=\"text-align: center;\">    0    </td><td style=\"text-align: center;\">      0      </td><td style=\"text-align: center;\">     -0.8     </td><td style=\"text-align: center;\">          0           </td><td style=\"text-align: center;\">     -0     </td></tr>\\n<tr><td style=\"text-align: center;\">    0    </td><td style=\"text-align: center;\">      0      </td><td style=\"text-align: center;\">     -0.6     </td><td style=\"text-align: center;\">          0           </td><td style=\"text-align: center;\">     -0     </td></tr>\\n<tr><td style=\"text-align: center;\">    0    </td><td style=\"text-align: center;\">      0      </td><td style=\"text-align: center;\">     0.2      </td><td style=\"text-align: center;\">          0           </td><td style=\"text-align: center;\">     -0     </td></tr>\\n<tr><td style=\"text-align: center;\">    0    </td><td style=\"text-align: center;\">     -1      </td><td style=\"text-align: center;\">     -0.8     </td><td style=\"text-align: center;\">          0           </td><td style=\"text-align: center;\">     -0     </td></tr>\\n<tr><td style=\"text-align: center;\">    0    </td><td style=\"text-align: center;\">      1      </td><td style=\"text-align: center;\">     -0.7     </td><td style=\"text-align: center;\">          0           </td><td style=\"text-align: center;\">     -0     </td></tr>\\n</tbody>\\n</table>'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pprint_train(item_train, item_features, ivs, i_s, maxcount=5, user=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train[:5]: [0. 9. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(f\"y_train[:5]: {y_train[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# scale training data\n",
    "if scaledata:\n",
    "    item_train_save = item_train\n",
    "    user_train_save = user_train\n",
    "\n",
    "    scalerItem = StandardScaler()\n",
    "    scalerItem.fit(item_train)\n",
    "    item_train = scalerItem.transform(item_train)\n",
    "\n",
    "    scalerUser = StandardScaler()\n",
    "    scalerUser.fit(user_train)\n",
    "    user_train = scalerUser.transform(user_train)\n",
    "\n",
    "    print(np.allclose(item_train_save, scalerItem.inverse_transform(item_train)))\n",
    "    print(np.allclose(user_train_save, scalerUser.inverse_transform(user_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Book/item training data shape: (66003, 5)\n",
      "Book/item test  data shape: (16501, 5)\n"
     ]
    }
   ],
   "source": [
    "item_train, item_test = train_test_split(item_train, train_size=0.80, shuffle=True, random_state=1)\n",
    "user_train, user_test = train_test_split(user_train, train_size=0.80, shuffle=True, random_state=1)\n",
    "y_train, y_test       = train_test_split(y_train,    train_size=0.80, shuffle=True, random_state=1)\n",
    "print(f\"Book/item training data shape: {item_train.shape}\")\n",
    "print(f\"Book/item test  data shape: {item_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th style=\"text-align: center;\"> [Use r-ID] </th><th style=\"text-align: center;\"> Loca tion </th><th style=\"text-align: center;\"> Age </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td style=\"text-align: center;\">     0      </td><td style=\"text-align: center;\">     0     </td><td style=\"text-align: center;\"> 0.3 </td></tr>\n",
       "<tr><td style=\"text-align: center;\">     1      </td><td style=\"text-align: center;\">     0     </td><td style=\"text-align: center;\">-1.3 </td></tr>\n",
       "<tr><td style=\"text-align: center;\">     0      </td><td style=\"text-align: center;\">     0     </td><td style=\"text-align: center;\"> 0.8 </td></tr>\n",
       "<tr><td style=\"text-align: center;\">     0      </td><td style=\"text-align: center;\">     0     </td><td style=\"text-align: center;\">-1.3 </td></tr>\n",
       "<tr><td style=\"text-align: center;\">     1      </td><td style=\"text-align: center;\">     0     </td><td style=\"text-align: center;\"> 0.1 </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "'<table>\\n<thead>\\n<tr><th style=\"text-align: center;\"> [Use r-ID] </th><th style=\"text-align: center;\"> Loca tion </th><th style=\"text-align: center;\"> Age </th></tr>\\n</thead>\\n<tbody>\\n<tr><td style=\"text-align: center;\">     0      </td><td style=\"text-align: center;\">     0     </td><td style=\"text-align: center;\"> 0.3 </td></tr>\\n<tr><td style=\"text-align: center;\">     1      </td><td style=\"text-align: center;\">     0     </td><td style=\"text-align: center;\">-1.3 </td></tr>\\n<tr><td style=\"text-align: center;\">     0      </td><td style=\"text-align: center;\">     0     </td><td style=\"text-align: center;\"> 0.8 </td></tr>\\n<tr><td style=\"text-align: center;\">     0      </td><td style=\"text-align: center;\">     0     </td><td style=\"text-align: center;\">-1.3 </td></tr>\\n<tr><td style=\"text-align: center;\">     1      </td><td style=\"text-align: center;\">     0     </td><td style=\"text-align: center;\"> 0.1 </td></tr>\\n</tbody>\\n</table>'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pprint_train(user_train, user_features, uvs, u_s, maxcount=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(66003, 1) (16501, 1)\n"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler((-1, 1))\n",
    "scaler.fit(y_train.reshape(-1, 1))\n",
    "ynorm_train = scaler.transform(y_train.reshape(-1, 1))\n",
    "ynorm_test = scaler.transform(y_test.reshape(-1, 1))\n",
    "print(ynorm_train.shape, ynorm_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_train shape: (66003, 2)\n",
      "item_train shape: (66003, 4)\n",
      "ynorm_train shape: (66003, 1)\n"
     ]
    }
   ],
   "source": [
    "print(f\"user_train shape: {user_train[:, u_s:].shape}\")\n",
    "print(f\"item_train shape: {item_train[:, i_s:].shape}\")\n",
    "print(f\"ynorm_train shape: {ynorm_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture\n",
    "\n",
    "The recommendation system uses a neural network with:\n",
    "1. User embedding layer\n",
    "2. Item embedding layer\n",
    "3. Dot product layer for rating prediction\n",
    "4. Regularization to prevent overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_17\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_17\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_20      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_22      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ sequential_10       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">37,792</span> │ input_layer_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ sequential_11       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">38,304</span> │ input_layer_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ sequential_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ sequential_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dot_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dot</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lambda_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│                     │                   │            │ lambda_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_20      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_22      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ sequential_10       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │     \u001b[38;5;34m37,792\u001b[0m │ input_layer_20[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mSequential\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ sequential_11       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │     \u001b[38;5;34m38,304\u001b[0m │ input_layer_22[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mSequential\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_10 (\u001b[38;5;33mLambda\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ sequential_10[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_11 (\u001b[38;5;33mLambda\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ sequential_11[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dot_5 (\u001b[38;5;33mDot\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ lambda_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│                     │                   │            │ lambda_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">76,096</span> (297.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m76,096\u001b[0m (297.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">76,096</span> (297.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m76,096\u001b[0m (297.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_outputs = 32\n",
    "\n",
    "tf.random.set_seed(1)\n",
    "user_NN = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(num_outputs, activation='linear'),\n",
    "])\n",
    "\n",
    "item_NN = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(num_outputs, activation='linear'),\n",
    "])\n",
    "\n",
    "# create the user input and point to the base network\n",
    "input_user = tf.keras.layers.Input(shape=(num_user_features,))\n",
    "vu = user_NN(input_user)\n",
    "vu = tf.keras.layers.Lambda(lambda x: tf.linalg.l2_normalize(x, axis=1))(vu)\n",
    "\n",
    "# create the item input and point to the base network\n",
    "input_item = tf.keras.layers.Input(shape=(num_item_features,))\n",
    "vm = item_NN(input_item)\n",
    "vm = tf.keras.layers.Lambda(lambda x: tf.linalg.l2_normalize(x, axis=1))(vm)\n",
    "\n",
    "# compute the dot product of the two vectors vu and vm\n",
    "output = tf.keras.layers.Dot(axes=1)([vu, vm])\n",
    "\n",
    "# specify the inputs and output of the model\n",
    "model = tf.keras.Model([input_user, input_item], output)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1)\n",
    "cost_fn = tf.keras.losses.MeanSquaredError()\n",
    "opt = keras.optimizers.Adam(learning_rate=0.01)\n",
    "model.compile(optimizer=opt,\n",
    "              loss=cost_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_train shape: (66003, 2)\n",
      "item_train shape: (66003, 4)\n",
      "ynorm_train shape: (66003, 1)\n"
     ]
    }
   ],
   "source": [
    "# Check the shapes of your data\n",
    "print(f\"user_train shape: {user_train[:, u_s:].shape}\")\n",
    "print(f\"item_train shape: {item_train[:, i_s:].shape}\")\n",
    "print(f\"ynorm_train shape: {ynorm_train.shape}\")\n",
    "\n",
    "# Make sure all arrays have the same number of samples\n",
    "min_samples = min(len(user_train), len(item_train), len(ynorm_train))\n",
    "user_train_trimmed = user_train[:min_samples, u_s:]\n",
    "item_train_trimmed = item_train[:min_samples, i_s:]\n",
    "ynorm_train_trimmed = ynorm_train[:min_samples]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "This section includes:\n",
    "1. Data preparation and scaling\n",
    "2. Training/test split\n",
    "3. Model training loop\n",
    "4. Loss tracking and optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m2063/2063\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - loss: 0.5829\n",
      "Epoch 2/30\n",
      "\u001b[1m2063/2063\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - loss: 0.5751\n",
      "Epoch 3/30\n",
      "\u001b[1m2063/2063\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - loss: 0.5742\n",
      "Epoch 4/30\n",
      "\u001b[1m2063/2063\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - loss: 0.5736\n",
      "Epoch 5/30\n",
      "\u001b[1m2063/2063\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - loss: 0.5730\n",
      "Epoch 6/30\n",
      "\u001b[1m2063/2063\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - loss: 0.5722\n",
      "Epoch 7/30\n",
      "\u001b[1m2063/2063\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - loss: 0.5718\n",
      "Epoch 8/30\n",
      "\u001b[1m2063/2063\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - loss: 0.5706\n",
      "Epoch 9/30\n",
      "\u001b[1m2063/2063\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - loss: 0.5708\n",
      "Epoch 10/30\n",
      "\u001b[1m2063/2063\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - loss: 0.5683\n",
      "Epoch 11/30\n",
      "\u001b[1m2063/2063\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - loss: 0.5670\n",
      "Epoch 12/30\n",
      "\u001b[1m2063/2063\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - loss: 0.5662\n",
      "Epoch 13/30\n",
      "\u001b[1m2063/2063\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - loss: 0.5686\n",
      "Epoch 14/30\n",
      "\u001b[1m2063/2063\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - loss: 0.5674\n",
      "Epoch 15/30\n",
      "\u001b[1m2063/2063\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 7ms/step - loss: 0.5679\n",
      "Epoch 16/30\n",
      "\u001b[1m2063/2063\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - loss: 0.5658\n",
      "Epoch 17/30\n",
      "\u001b[1m2063/2063\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - loss: 0.5666\n",
      "Epoch 18/30\n",
      "\u001b[1m2063/2063\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - loss: 0.5666\n",
      "Epoch 19/30\n",
      "\u001b[1m2063/2063\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - loss: 0.5654\n",
      "Epoch 20/30\n",
      "\u001b[1m2063/2063\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - loss: 0.5646\n",
      "Epoch 21/30\n",
      "\u001b[1m2063/2063\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - loss: 0.5646\n",
      "Epoch 22/30\n",
      "\u001b[1m2063/2063\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - loss: 0.5639\n",
      "Epoch 23/30\n",
      "\u001b[1m2063/2063\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - loss: 0.5631\n",
      "Epoch 24/30\n",
      "\u001b[1m2063/2063\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - loss: 0.5632\n",
      "Epoch 25/30\n",
      "\u001b[1m2063/2063\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - loss: 0.5623\n",
      "Epoch 26/30\n",
      "\u001b[1m2063/2063\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - loss: 0.5630\n",
      "Epoch 27/30\n",
      "\u001b[1m2063/2063\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - loss: 0.5631\n",
      "Epoch 28/30\n",
      "\u001b[1m2063/2063\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - loss: 0.5624\n",
      "Epoch 29/30\n",
      "\u001b[1m2063/2063\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - loss: 0.5631\n",
      "Epoch 30/30\n",
      "\u001b[1m2063/2063\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - loss: 0.5615\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1bf8616d250>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now fit the model with the trimmed data\n",
    "tf.random.set_seed(1)\n",
    "model.fit([user_train_trimmed, item_train_trimmed], ynorm_train_trimmed, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.5677\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5703388452529907"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate([user_test[:, u_s:], item_test[:, i_s:]], ynorm_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation and Recommendations\n",
    "\n",
    "This section includes:\n",
    "1. Model evaluation metrics\n",
    "2. Function to generate personalized book recommendations\n",
    "3. Example recommendations for specific users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New user vector replicated shape: (103130, 2)\n",
      "Number of items: 103130\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Prepare the new user vector and replicate it for all books\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# For our book model the user features (as used for training) are:\n",
    "# ['Location', 'Age'] (user ID is not used for the neural network input)\n",
    "# Here, we define a new user with arbitrary values.\n",
    "new_location = 2   # for example, the factorized value for location\n",
    "new_age = 30       # the new user's age\n",
    "\n",
    "# Create the new user vector (only the features required, shape: [1, 2])\n",
    "new_user_vec = np.array([[new_location, new_age]])\n",
    "\n",
    "# Load the precomputed item (book) vectors\n",
    "# These vectors were generated from your content_item_train.csv and saved in content_item_vecs.csv.\n",
    "item_vecs = np.loadtxt('data/content_item_vecs.csv', delimiter=',')\n",
    "num_items = item_vecs.shape[0]\n",
    "\n",
    "# Replicate the new user vector to match each item.\n",
    "# This creates an array of shape (num_items, 2)\n",
    "new_user_vecs = np.tile(new_user_vec, (num_items, 1))\n",
    "\n",
    "print(f\"New user vector replicated shape: {new_user_vecs.shape}\")\n",
    "print(f\"Number of items: {num_items}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3223/3223\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step\n",
      "Number of item vectors: 103130\n",
      "Number of books in book_list: 60137\n",
      "Top recommended books for the new user:\n",
      "ISBN: 1569871205, Title: The Mutiny on the H. M. S. Bounty (Illustrated Classics), Author: William Bligh, Predicted Rating: 0.136\n",
      "ISBN: 0451170857, Title: A Cat in Wolf's Clothing (Alice Nestleton Mysteries (Paperback)), Author: Lydia Adamson, Predicted Rating: 0.133\n",
      "ISBN: 0425186105, Title: Gladiatrix: The True Story of History's Unknown Woman Warrior, Author: Amy Zoll, Predicted Rating: 0.133\n",
      "ISBN: 3499264293, Title: Ein unverhofftes GestÃ?Â¤ndnis. Roman., Author: P. D. James, Predicted Rating: 0.132\n",
      "ISBN: 0446607223, Title: Shadows on the Aegean, Author: Suzanne Frank, Predicted Rating: 0.132\n",
      "ISBN: 0553272055, Title: Call Me Anna: The Autobiography of Patty Duke, Author: Patty Duke, Predicted Rating: 0.132\n",
      "ISBN: 0312876637, Title: The Folk of the Fringe, Author: Orson Scott Card, Predicted Rating: 0.132\n",
      "ISBN: 0965504786, Title: Secret Origins of the Bible, Author: Tim Callahan, Predicted Rating: 0.132\n",
      "ISBN: 002012600X, Title: Angry Young Sniglets (Snig'lit : Any Word That Doesn't Appear in the Dictionary, But Should), Author: Rich Hall, Predicted Rating: 0.132\n",
      "ISBN: 3257062710, Title: Feine Freunde. Commissario Brunettis neunter Fall., Author: Donna Leon, Predicted Rating: 0.132\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Predict ratings using the trained model.\n",
    "predictions = model.predict([new_user_vecs, item_vecs]).flatten()\n",
    "\n",
    "# Sort predictions in descending order.\n",
    "sorted_indices = np.argsort(predictions)[::-1]\n",
    "\n",
    "# Load the book list.\n",
    "book_list = pd.read_csv('data/content_book_list.csv')\n",
    "\n",
    "print(\"Number of item vectors:\", item_vecs.shape[0])\n",
    "print(\"Number of books in book_list:\", len(book_list))\n",
    "\n",
    "# Filter indices if necessary\n",
    "valid_top_indices = [idx for idx in sorted_indices if idx < len(book_list)]\n",
    "top_n = 10\n",
    "top_indices = valid_top_indices[:top_n]\n",
    "top_predictions = predictions[top_indices]\n",
    "\n",
    "print(\"Top recommended books for the new user:\")\n",
    "for idx, pred in zip(top_indices, top_predictions):\n",
    "    isbn = book_list.iloc[idx]['ISBN']\n",
    "    title = book_list.iloc[idx]['Book-Title']\n",
    "    author = book_list.iloc[idx]['Book-Author']\n",
    "    print(f\"ISBN: {isbn}, Title: {title}, Author: {author}, Predicted Rating: {pred:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
